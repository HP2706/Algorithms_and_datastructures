\documentclass{article}
\usepackage{amsfonts}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{float}
\usepackage{bbm}
\usepackage[round]{natbib}
\usepackage{hyperref}
\usepackage{graphicx} % Required for inserting images
\usepackage{tikz}
\usepackage{parskip} % Add this to remove paragraph indentation
\usetikzlibrary{shapes,arrows,positioning,fit}
\usepackage{listings}
\usepackage{amsthm}  % Add this package for theorem-like environments
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{mathabx}
\usepackage{subcaption}  % For subfigures
\usepackage{algorithm}
\usepackage{algpseudocode}
\bibliographystyle{plain}

% Define theorem-like environments
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}


\begin{document}

\section{Definitions}


\begin{remark}
We are mostly interested in functions $f$ from $\mathbb{N}$ to $\mathbb{R}_+ = [0,\infty)$.
\end{remark}

\begin{definition}[Big O]
Let $f,g : \mathbb{N} \to \mathbb{R}_+$. We write $f(n) \leq O(g(n))$ if there are constants $C,n_0 > 0$ so that for all $n \geq n_0$,
\[ f(n) \leq Cg(n). \]
\end{definition}

\begin{definition}[Big $\Omega$]
Let $f,g : \mathbb{N} \to \mathbb{R}_+$. We write $f(n) \geq \Omega(g(n))$ if there are constants $c,n_0 > 0$ so that for all $n \geq n_0$,
\[ f(n) \geq cg(n). \]
\end{definition}

\begin{remark}
Comparing Big O and Big $\Omega$ notation:
\begin{itemize}
    \item Big O provides an upper bound on the growth rate of a function
    \item Big $\Omega$ provides a lower bound on the growth rate of a function
    \item The constants $C$ and $c$ in the definitions represent scaling factors
    \item Both definitions require the inequality to hold for all values beyond some initial point $n_0$
    \item If a function is both $O(g(n))$ and $\Omega(g(n))$, we say it is $\Theta(g(n))$, meaning it grows at exactly the same rate as $g(n)$
\end{itemize}
\end{remark}

\begin{remark}
The following proof demonstrates the equivalence between $f(n) \leq O(g(n))$ and $g(n) = \Omega(f(n))$, showing these are just different ways of expressing the same relationship between functions.
\end{remark}

\begin{proof}
($\Rightarrow$) First, assume $f(n) \leq O(g(n))$. By definition, this means there exist constants $C, n_0 > 0$ such that for all $n \geq n_0$:
\[ f(n) \leq Cg(n) \]

Rearranging this inequality:
\[ g(n) \geq \frac{1}{C}f(n) \]

Let $c = \frac{1}{C}$. Since $C > 0$, we have $c > 0$. Therefore, there exists $c > 0$ and $n_0 > 0$ such that for all $n \geq n_0$:
\[ g(n) \geq cf(n) \]

This is precisely the definition of $g(n) = \Omega(f(n))$.

($\Leftarrow$) Conversely, assume $g(n) = \Omega(f(n))$. By definition, this means there exist constants $c, n_0 > 0$ such that for all $n \geq n_0$:
\[ g(n) \geq cf(n) \]

Rearranging:
\[ f(n) \leq \frac{1}{c}g(n) \]

Let $C = \frac{1}{c}$. Since $c > 0$, we have $C > 0$. Therefore, there exists $C > 0$ and $n_0 > 0$ such that for all $n \geq n_0$:
\[ f(n) \leq Cg(n) \]

This is precisely the definition of $f(n) \leq O(g(n))$.

Therefore, $f(n) \leq O(g(n))$ if and only if $g(n) = \Omega(f(n))$. \qed
\end{proof}

\section{Proofs}


    \begin{align}
        &\text{Proof by Strong Induction:} \notag \\[1em]
        &\text{Base case: } n = 2 \text{ is prime, so } n = \prod_{i=1}^1 p_i \text{ where } p_1 = 2. \notag \\[1em]
        &\text{Inductive Hypothesis: Assume the statement holds for all natural numbers } 2,3,\ldots,k. \notag \\[1em]
        &\text{Consider } k+1: \notag \\
        &\text{Case 1: If } k+1 \text{ is prime, then } k+1 = \prod_{i=1}^1 p_i \text{ where } p_1 = k+1. \notag \\[1em]
        &\text{Case 2: If } k+1 \text{ is composite, then } k+1 = a \cdot b \text{ where } 1 < a,b < k+1. \notag \\
        &\text{By the inductive hypothesis, } a = \prod_{i=1}^m p_i \text{ and } b = \prod_{j=1}^n q_j \text{ for some primes } p_i, q_j. \notag \\
        &\text{Therefore, } k+1 = a \cdot b = \left(\prod_{i=1}^m p_i\right) \cdot \left(\prod_{j=1}^n q_j\right) = \prod_{l=1}^{m+n} r_l \notag \\
        &\text{where } r_l \text{ represents the combined sequence of primes } p_i \text{ and } q_j. \notag \\[1em]
        &\text{By the principle of strong induction, the statement holds for all } n > 1. \quad \square \notag
    \end{align}

\section{Exercises 5/2}

\begin{enumerate}
    \item Giv en formel for summen af de første $n$ ulige tal:
    \[ S(n) = \sum_{i=1}^n (2i-1) \]

    \[
    \sum_{i=1}^n (2i-1) = -n + 2*\sum_{i=1}^n (i) = -n + \frac{2*(n(n+1))}{2} = -n + n^2 + n = n^2
    \]

    \item Vis at $n! \geq 2^n$ for $n \geq 4$.
    
    vi viser dette ved induktion.
    \begin{enumerate}
        \item base case: n=4
        $4! = 4*3*2*1=24$
        $2^4 = 16$
        $4! \geq 2^4$

        \item Vi anvender induktionshypotesen $n! \geq 2^n$
        
        For $n+1$ har vi:
        \begin{align*}
            (n+1)! &= (n+1) \cdot n! \\
            &\geq (n+1) \cdot 2^n \quad \text{(fra induktionshypotesen)} \\
            &> 2 \cdot 2^n \quad \text{(da $n+1 > 2$ for $n \geq 4$)} \\
            &= 2^{n+1}
        \end{align*}
    \end{enumerate}

    \item Fibonacci tallene defineres som $f_0 = 0$, $f_1 = 1$, $f_i = f_{i-1} + f_{i-2}$ for $i \geq 2$. Bevis at:
    \[ \sum_{i=1}^n f_i^2 = f_n f_{n+1} \]

    vi viser det ved induktion
    \begin{enumerate}
        \item base case: n=2
        \[ \sum_{i=1}^2 f_i^2 = f_1^2 + f_2^2 = 1^2 + 1^2 = f_2 f_{3} = 1*2 = 2 \]
        \item induktion step
        \[ \sum_{i=1}^{n+1} f_i^2 = f_{n+1}^2 + \sum_{i=1}^n f_i^2 = f_{n+1}^2 + f_n f_{n+1} = f_{n+1}(f_{n+1} + f_{n}) = f_{n+1}f_{n+2}\]
    \end{enumerate}

    \item Vis at:
    \[ \sum_{i=1}^n \frac{1}{i(i+1)} = \frac{n}{n+1} \]

     vi viser det ved induktion
    \begin{enumerate}
        \item base case: n=2
        \[ \sum_{i=1}^2 \frac{1}{i(i+1)} = \frac{1}{2}+\frac{1}{6} = \frac{4}{6} = \frac{2}{2+1}=\frac{2}{3}\]
        \item induktion step
        \[
        \sum_{i=1}^{(n+1)} \frac{1}{i(i+1)} =  \frac{1}{(n+1)((n+2)} + \sum_{i=1}^n \frac{1}{i(i+1)} = \frac{1}{(n+1)((n+2)} + \frac{n}{n+1} =\] 
        
        \[
        \frac{1}{(n+1)(n+2)} + \frac{n(n+2)}{(n+1)(n+2)} = \frac{1+n(n+2)}{(n+1)(n+2)} =  \frac{(n+1)(n+1)}{(n+1)(n+2)} = \frac{n+1}{n+2}
        \]
    \end{enumerate}
\end{enumerate}

\subsection{Opgave 8}
\subsection{Løkkeinvarianter}

Vi skal nu bruge induktion til at bevise korrekthed af programmer (specifikt løkker). Vi kigger på følgende algoritme:

\begin{algorithm}
\caption{Marbles}
\begin{algorithmic}[1]
\State $i \leftarrow n$
\While{$i \geq 1$}
    \State Pick two arbitrary marbles $m_1, m_2$ from the jar.
    \If{Color$(m_1)$ = Color$(m_2)$}
        \State Throw the two marbles away
        \State Place a RED marble in the jar.
    \Else
        \State Throw away the RED marble
        \State Put the BLUE marble back in the jar.
    \EndIf
    \State $i \leftarrow i-1$
\EndWhile
\end{algorithmic}
\end{algorithm}

Vi ønsker at argumentere om følgende:

\begin{enumerate}
    \item Algorithm 1 terminerer med køretid $O(n)$.
    
    For at bevise at algoritmen terminerer med køretid $O(n)$, kan vi observere følgende:

    \begin{enumerate}
    \item \textbf{Løkkeinvariant:} Ved starten af hver iteration er $i$ altid mindre end værdien af $i$ i forrige iteration.
    
        \begin{enumerate}
            \item \textbf{Initialisation:} $i$ starter med værdien $n$.
            
            \item \textbf{Vedligeholdelse:} I hver iteration decrementeres $i$ med 1 (linje 11: $i \leftarrow i-1$).
            
            \item \textbf{Terminering:} Løkken fortsætter så længe $i \geq 1$. Da $i$ reduceres med 1 i hver iteration, 
            og starter fra $n$, vil løkken køre præcis $n$ gange før $i < 1$.
        \end{enumerate}

    \item Der er netop 1 kugle tilbage i urnen, hvad er farven af den sidste kugle? TODO
    
    den vil altid være rød. 
    3 muligheder.
    rød, rød -> en rød
    rød, blå -> blå, blå -> rød
    blå, blå -> rød
\end{enumerate}

\end{enumerate}

\subsection{Løkkeinvarianter 2024 eksamen opgave 4 (4\%)}

Betragt følgende funktion, i pseudo-kode notationen fra CLRS, hvor input $A[1:n]$ er et array af heltal og $n > 1$ er længden af $A$.

\begin{algorithm}
\caption{InTheLoop}
\begin{algorithmic}[1]
\State $a \leftarrow \max\{A[1], A[2]\}$
\State $b \leftarrow \min\{A[1], A[2]\}$
\For{$i = 3$ to $n$}
    \If{$A[i] \geq a$}
        \State $b \leftarrow a$
        \State $a \leftarrow A[i]$
    \ElsIf{$A[i] \geq b$}
        \State $b \leftarrow A[i]$
    \EndIf
\EndFor
\State \Return $b$
\end{algorithmic}
\end{algorithm}

\textbf{Eksempel:} For $n = 5$ og $A = \langle 6,4,-2,6,7 \rangle$ returnerer InTheLoop værdien 6.

Hvilke udsagn er gyldige invarianter, der er sande umiddelbart efter udførelsen if-sætningen i linje 4-8 (\emph{inden} variablen $i$ bliver øget med 1)? Vælg ét eller flere korrekte svar.

\begin{enumerate}
    \item $a \geq 0$
    \item $b \geq 0$
    \item $a \geq b$ true
    \item $a > b$
    \item $a = \max_{1 \leq j \leq i} A[j]$ True
    \item $b = \min_{1 \leq j \leq i} A[j]$
\end{enumerate}

\subsection{2024 eksamen opgave 20: Induktionsbevis for køretid (8\%)}

En algoritme tager tid $T(n) = n^2+\sum_{i=1}^n \binom{i+1}{2}$ på et input af størrelse $n$, hvor $\binom{i+1}{2} = \frac{i(i+1)}{2}$.
Vi ønsker at vise at køretiden overholder $T(n) \leq 2n^3$ for alle $n \geq 1$.

\begin{enumerate}
    \item Vis at $T(1) = 2$. Skriv dit svar på side 3 i Word dokumentet.
    
    $T(1) = 1^2 + \sum_{i=1}^1 \binom{i+1}{2} = 1^2 + 1 = 2$
    
    \item Vis $T(n) \leq 2n^3$ ved induktion. Skriv dit svar på side 4 i Word dokumentet.
    
    $T(n+1) = (n+1)^2+\sum_{i=1}^{(n+1)} \frac{i(i+1)}{2}$
    $= (n+1)^2 + \sum_{i=1}^{n+1} \frac{i(i+1)}{2}$
    $\leq (n+1)^2 + \sum_{i=1}^{n+1} i^2$ (since $\frac{i(i+1)}{2} \leq i^2$ for $i \geq 1$)
    $= (n+1)^2 + \frac{(n+1)(n+2)(2n+3)}{6}$ (using sum of squares formula)
    $\leq (n+1)^2 + (n+1)^3$ (since $\frac{(n+2)(2n+3)}{6} \leq (n+1)^2$ for $n \geq 1$)
    $\leq 2(n+1)^3$ (since $(n+1)^2 \leq (n+1)^3$ for $n \geq 1$)

    Therefore, by induction, $T(n) \leq 2n^3$ for all $n \geq 1$. \qed
\end{enumerate}

\subsection{Løkkeinvarianter 2024 april eksamen opgave 4 (4\%)}

Betragt følgende funktion, i pseudo-kode notationen fra CLRS, hvor input $A$ er et array af reelle tal og $n$ er længden af $A$.

\begin{algorithm}
\caption{Variance}
\begin{algorithmic}[1]
\State $r \leftarrow 0$
\State $s \leftarrow 0$
\For{$i = 1$ to $n$}
    \State $r \leftarrow r + A[i] \cdot A[i]$
    \State $s \leftarrow s + A[i]$
\EndFor
\State \Return $r/n - (s \cdot s)/(n \cdot n)$
\end{algorithmic}
\end{algorithm}

\textbf{Eksempel:} For $n = 4$ og $A = \langle \frac{1}{2}, 2, 0, -\frac{1}{2} \rangle$ returnerer VARIANCE værdien $7/8$.

Hvilke udsagn er gyldige løkke-invarianter, der er sande efter udførelsen af linje 5 i for-løkken? Vælg ét eller flere korrekte svar.

\begin{enumerate}
    \item $s \leq r$ forkert da $A[i]^2 \leq A[i]$ for $A[i] \in [0,1)$ 
    \item $s \geq 0$ forkert
    \item $r \geq 0$ korrekt, da $A[i]^2 \in \mathbb{R}_{+}$ 
    \item $i \geq 0$ sand $i \in [1, n]$
    \item $s = \sum_{j=1}^n A[j]$ forkert 
    \item $r = \sum_{j=1}^i A[j]^2$ korrekt
    \item $i \leq n$ korrekt 
\end{enumerate}


\subsection{Induktionsbeviser (7\%)}
Betragt følgende funktion, i pseudo-kode notationen fra CLRS, hvor input $A$ er et array af heltal, $n$ er længden af $A$, og $b$ er en heltalsparameter.

\begin{algorithm}
\caption{HeavyHitter}
\begin{algorithmic}[1]
\State let $r[1:b]$ be a new array
\For{$i = 1$ to $b$}
    \State $r[i] = 0$
\EndFor
\For{$j = 1$ to $n$}
    \If{$A[j] \geq 1$ and $A[j] \leq b$}
        \State $r[A[j]] = r[A[j]] + 1$
    \EndIf
\EndFor
\State \Return $\max_{1 \leq \ell \leq b}(r[\ell])$
\end{algorithmic}
\end{algorithm}

\textbf{Eksempel:} For $n = 6$, $b = 10$, $A = \langle 5,1,1,7,1,3 \rangle$ returnerer HeavyHitter værdien 3.

a) Forklar i ord hvad HeavyHitter beregner. 

HeavyHitter beregner frekvensen af det tal, der forekommer flest gange i array A, men kun blandt tal i intervallet [1,b]. Algoritmen:
1. Opretter et tællerarray r af længde b
2. For hvert element i A, hvis elementet er mellem 1 og b (inklusiv), øges den tilsvarende tæller i r
3. Returnerer den højeste frekvens (største tællerværdi) fundet i r

For eksempel, givet A = ⟨5,1,1,7,1,3⟩ og b = 10:
- Tallet 1 forekommer 3 gange
- Tallet 3 forekommer 1 gang
- Tallet 5 forekommer 1 gang
- Tallet 7 forekommer 1 gang
- Alle andre tal forekommer 0 gange
Derfor returneres 3, som er den højeste frekvens.

b) Løkkeinvariant $I(j)$: For alle $\ell \in [1,b]$ gælder at $r[\ell]$ er lig med antallet af forekomster af værdien $\ell$ blandt de første $j$ elementer i $A$ (dvs. $A[1..j]$).

Bevis ved induktion:

\textbf{Basis} ($j = 0$):
Før løkken starter er $r[\ell] = 0$ for alle $\ell \in [1,b]$, hvilket er korrekt da vi endnu ikke har set nogen elementer fra $A$.

\textbf{Induktionsskridt}:
Antag at $I(k)$ gælder for et $k \geq 0$. Vi skal vise at $I(k+1)$ også gælder.

Ved iteration $k+1$ undersøges $A[k+1]$:
\begin{itemize}
    \item Hvis $1 \leq A[k+1] \leq b$: $r[A[k+1]]$ øges med 1, hvilket korrekt tæller den nye forekomst
    \item Hvis $A[k+1] < 1$ eller $A[k+1] > b$: $r$ forbliver uændret, hvilket er korrekt da vi kun tæller værdier i intervallet $[1,b]$
\end{itemize}

Alle andre positioner i $r$ forbliver uændrede. Derfor vedligeholder iterationen invarianten.

\textbf{Konklusion}:
Når løkken terminerer gælder $I(n)$, hvilket betyder at $r[\ell]$ indeholder det totale antal forekomster af værdien $\ell$ i hele array $A$. Dette understøtter at $\max_{1 \leq \ell \leq b}(r[\ell])$ returnerer frekvensen af det hyppigst forekommende tal i intervallet $[1,b]$.


\section{Exercises 10/2}
CLRS problem 3-4, 3-2, 3-1. 


\subsection{3-1 Asymptotic behavior of polynomials}
Let
\[
p(n) = \sum_{i=0}^{d} a_i n^i,
\]
where \( a_d > 0 \), be a degree-\( d \) polynomial in \( n \), and let \( k \) be a constant. Use the definitions of the asymptotic notations to prove the following properties.

First, let's recall the formal definitions of the asymptotic notations:

\begin{definition}[Big O]
For a given function $g(n)$, we denote by $O(g(n))$ the set of functions:
\[ O(g(n)) = \{f(n): \text{ there exist positive constants } c \text{ and } n_0 \text{ such that } 0 \leq f(n) \leq cg(n) \text{ for all } n \geq n_0\} \]
\end{definition}

\begin{definition}[Big $\Omega$]
For a given function $g(n)$, we denote by $\Omega(g(n))$ the set of functions:
\[ \Omega(g(n)) = \{f(n): \text{ there exist positive constants } c \text{ and } n_0 \text{ such that } 0 \leq cg(n) \leq f(n) \text{ for all } n \geq n_0\} \]
\end{definition}

\begin{definition}[Big $\Theta$]
For a given function $g(n)$, we denote by $\Theta(g(n))$ the set of functions:
\[ \Theta(g(n)) = \{f(n): \text{ there exist positive constants } c_1, c_2 \text{ and } n_0 \text{ such that } 0 \leq c_1g(n) \leq f(n) \leq c_2g(n) \text{ for all } n \geq n_0\} \]
\end{definition}

Note that $f(n) = \Theta(g(n))$ if and only if $f(n) = O(g(n))$ and $f(n) = \Omega(g(n))$.


\begin{enumerate}
    \item[a.] If \( k \geq d \), then \( p(n) = O(n^k) \).

    \begin{proof}
    Let \( p(n) = \sum_{i=0}^d a_i n^i \) where \( a_d > 0 \). We need to show there exist constants \( C > 0 \) and \( n_0 \) such that \( p(n) \leq Cn^k \) for all \( n \geq n_0 \).

    For any \( n \geq 1 \):
    \[ p(n) = \sum_{i=0}^d a_i n^i = a_d n^d + \sum_{i=0}^{d-1} a_i n^i \]

    Since \( k \geq d \), we have \( n^d \leq n^k \). Therefore:
    \[ p(n) \leq a_d n^k + \sum_{i=0}^{d-1} |a_i| n^i \]

    For \( n \geq 1 \), we have \( n^i \leq n^k \) for all \( i \leq d \leq k \). Thus:
    \[ p(n) \leq a_d n^k + n^k \sum_{i=0}^{d-1} |a_i| = \left(a_d + \sum_{i=0}^{d-1} |a_i|\right)n^k \]

    Let \( C = a_d + \sum_{i=0}^{d-1} |a_i| \) and \( n_0 = 1 \). Then for all \( n \geq n_0 \):
    \[ p(n) \leq Cn^k \]

    Therefore, \( p(n) = O(n^k) \).
    \end{proof}
    \item[b.] If \( k \leq d \), then \( p(n) = \Omega(n^k) \).
    \begin{proof}

    For any \( n \geq 1 \):
    \[ p(n) = \sum_{i=0}^d a_i n^i = a_d n^d + \sum_{i=0}^{d-1} a_i n^i \]

    Since \( k \leq d \), we have \( n^d \geq n^k \). Therefore:
    \[ p(n) \geq a_d n^k + \sum_{i=0}^{d-1} |a_i| n^i \]

    For \( n \geq 1 \), we have \( n^i \geq n^k \) for all \( i \leq k \leq d \). Thus:
    \[ p(n) \geq a_d n^k + n^k \sum_{i=0}^{d-1} |a_i| = \left(a_d + \sum_{i=0}^{d-1} |a_i|\right)n^k \]

    Let \( C = a_d + \sum_{i=0}^{d-1} |a_i| \) and \( n_0 = 1 \). Then for all \( n \geq n_0 \):
    \[ p(n) \geq Cn^k \]    
    \end{proof}

    \item[c.] If \( k = d \), then \( p(n) = \Theta(n^k) \).
    
    \begin{proof}

        For any \( n \geq 1 \):
        \[ p(n) = \sum_{i=0}^d a_i n^i = a_d n^d + \sum_{i=0}^{d-1} a_i n^i \]
    
        Since \( k = d \), we have \( n^d = n^k \). Therefore:
        \[ p(n) = a_d n^k + \sum_{i=0}^{d-1} a_i n^i \] 

        thus 
        \[ a_d n^k + \sum_{i=0}^{d-1} a_i n^i \leq (a_d + \sum_{i=0}^{d-1} a_i)  n^k = c_1 * n^k\]
        this implies \[p(n) \leq c_1 * n^k \forall n \geq n_0\] and thus \[
        p(n) = \omega(n^k)\]
        and by the same argument
        \[ a_d n^k + \sum_{i=0}^{d-1} a_i n^i \geq (a_d + \sum_{i=0}^{d-1} |a_i|)  n^k = c_2 * n^k\] this implies \[p(n) \geq c_2 * n^k \forall n \geq n_0\] and thus \[
        p(n) = O(n^k)\]
        
        \[p(n) = O(n^k) = \Omega(n^k) \implies p(n) = \Theta(n^k)\] 


    \end{proof}
    
    \item[d.] If \( k > d \), then \( p(n) = o(n^k) \).
    same argument as by A
    \item[e.] If \( k < d \), then \( p(n) = \omega(n^k) \).
    same argument as by B
\end{enumerate}

\subsection{3-2 Relative asymptotic growths}
Indicate, for each pair of expressions (\( A, B \)) in the table below whether \( A \) is \( O \), \( o \), \( \Omega \), \( \omega \), or \( \Theta \) of \( B \). Assume that \( k \geq 1 \), \( \epsilon > 0 \), and \( c > 1 \) are constants. Write your answer in the form of the table with "yes" or "no" written in each box.

\[
\begin{array}{|c|c|c|c|c|c|c|}
\hline
A & B & O & o & \Omega & \omega & \Theta \\
\hline
\log^k n & n^\epsilon & x &   &  &  &  \\
n^k & c^n & x &  &  &  &  \\
\sqrt{n} & n^{\sin n} &  &  &  &  &  x\\
2^n & 2^{n/2} &  &  &  & x &  \\
n^{\log c} & c^{\log n} &  &  &  &  & x \\
\log(n!) & \log(n^n) &  x &  &  &  &  \\
\hline
\end{array}
\]

\subsection{3-4 Asymptotic notation properties}
Let \( f(n) \) and \( g(n) \) be asymptotically nonnegative functions. Prove or disprove each of the following conjectures.

\begin{enumerate}
    \item[a.] \( f(n) = O(g(n)) \) implies \( g(n) = \Omega(f(n)) \).
    if \( f(n) = O(g(n)) \) then $\exists n_0 \in \mathbb{N}, c \in \mathbb{R}$ st $f(n) \leq c*g(n) \forall n \geq n_0$ and thus $c*f(n) \leq g(n) \forall n \geq n_0$ and thus the statement is proven.
    \item[b.] \( f(n) + g(n) = \Theta(\min \{ f(n), g(n) \}) \).
    this is false... TODO
    \item[c.] \( f(n) = O(g(n)) \) implies \( \log f(n) = O(\log g(n)) \).
    This is false. Consider the counterexample:
    Let \( f(n) = n^2 \) and \( g(n) = n \). Then \( f(n) = O(g(n)^2) \), so \( f(n) = O(g(n)) \).
    However, \( \log f(n) = \log(n^2) = 2\log(n) \) is not \( O(\log g(n)) = O(\log n) \).
    \item[d.] \( f(n) = O(g(n)) \) implies \( 2^{f(n)} = O(2^{g(n)}) \).
    yes as by definition $\exists c > 0 \in R, \exists n_0 \in N$
    \( f(n) = c*g(n)\) and since $a > b \implies 2^a > 2^b$
    \item[e.] \( f(n) = O(g(n)) \) implies \( g(n) = O(f(n)) \).
    no consider $f(n) = n^2, g(n) = n^3$ then
    $f(n) = O(g(n))$ but $g(n) \neq O(f(n))$ since $n^3 \geq n^2$ for all c and $n_0$
    \item[f.] \( f(n) = O(g(n)) \) implies \( g(n) = O(2^{f(n)}) \).
    no, consider $f(n) = \frac{1}{n}, g(n) = n^2$ then $f(n) = O(g(n))$ but $2^n \neq O(2^{\frac{1}{n}}) $
    \item[g.] \( f(n) = \Theta(g(n)) \). it depends
    \item[h.] \( f(n) + o(f(n)) = \Theta(f(n)) \). no
\end{enumerate}



\subsection{Algoritmeanalyse I (4\%)(opgave 5, exam june 2024)}
Betragt denne algoritme der tager et array $A[1:n]$ af heltal som input og returnerer et array $B[1:n]$.

\begin{algorithm}
\caption{MinAlgoritme}
\begin{algorithmic}[1]
\State Let $B[1:n]$ be a new array
\State $k = 1$
\For{$i = 1$ to $n$}
    \If{$A[i] < A[k]$}
        \State $k = i$
    \EndIf
    \State $B[i] = A[k]$
\EndFor
\State \Return $B$
\end{algorithmic}
\end{algorithm}

Hvilke af følgende udsagn gælder altid om $B$, der returneres af MinAlgoritme$(A,n)$, uanset indholdet af $A$? Vælg ét eller flere korrekte svar.

\begin{enumerate}
    \item Sekvensen $B[1],\ldots,B[n]$ er ikke-aftagende. forkert(hvis vi tæller 0 med)
    \item Sekvensen $B[1],\ldots,B[n]$ er ikke-voksende. korrekt
    \item $B$ indeholder en delmængde tallene fra $A$. korrekt
    \item $B$ indeholder det største tal fra $A$. forket
    \item For hvert $i \in \{1,\ldots,n\}$ gælder $B[i] = \min_{1\leq j\leq i} A[j]$. korrekt
    \item For hvert $i \in \{1,\ldots,n\}$ gælder $B[i] = \min_{i\leq j\leq n} A[j]$. forkert
\end{enumerate}

\subsection{Algoritmeanalyse II (4\%) (opgave 6, exam june 2024)}
Betragt denne algoritme der som argument tager et array $A[1:n]$ med heltal. Algoritmen ændrer på indeholdet i $A$ (call by reference), i stil med QuickSort.

\begin{algorithm}
\caption{SortHumor}
\begin{algorithmic}[1]
\If{$n > 1$}
    \State SortHumor$(A, n-1)$
    \If{$A[n-1] > A[n]$}
        \State exchange $A[n-1]$ with $A[n]$
    \EndIf
    \State SortHumor$(A, n-1)$
\EndIf
\end{algorithmic}
\end{algorithm}

Hvilke af følgende udsagn gælder? Vælg ét eller flere korrekte svar.
\begin{enumerate}
    \item SortHumor sorterer $A$ i ikke-aftagende rækkefølge. korrekt
    \item SortHumor sorterer $A$ i ikke-voksende rækkefølge. forkert
    \item Køretiden af SortHumor er $\Theta(n\log n)$. forkert 
    \item Køretiden af SortHumor er $\Theta(n^2)$. forkert
    \item Køretiden af SortHumor er $\Theta(2^n)$. sandt
\end{enumerate}

\subsection{Køretid (9\%)(opgave 21 exam june 2024)}
Antag i det følgende at $A[1:n]$ er et array af $n$ heltal og at $k$ er et heltal. Vi betragter disse to funktioner:

\begin{algorithm}
\caption{PrefixSum}
\begin{algorithmic}[1]
\State $s = 0$
\If{$k > n$}
    \State $t = n$
\Else
    \State $t = k$
\EndIf
\For{$i = 1$ to $t$}
    \State $s = s + A[i]$
\EndFor
\State \Return $s$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{PowerSums}
\begin{algorithmic}[1]
\State $k = 1$
\State $s = 0$
\While{$k < n$}
    \State $s = s + \text{PrefixSum}(A, n, k)$
    \State $k = 2 \cdot k$
\EndWhile
\State \Return $s$
\end{algorithmic}
\end{algorithm}

\begin{enumerate}
    \item Angiv en asymptotisk grænse på køretiden for PrefixSum med brug af store-$\Theta$ notation, som funktion af variablerne $n$ og $k$. Argumentér kort for dit svar.
    
    siden prefix sum løber over min(n, k) elementer. køretiden for prefix sum er $\Theta(min(n, k))$

    \item Angiv antal iterationer for while løkken i PowerSums med brug af store-$\Theta$ notation, som funktion af $n$. Argumentér kort for dit svar.
    
    $\Theta(log n)$

    \item Angiv en øvre grænse for køretiden af PowerSums og udtryk den i store-$\Theta$ notation, som en simpel funktion af $n$. Argumentér kort for dit svar.
    
    we do the following amount of work

    $\sum_{i=0}^{log(n)} 2^i = 2^{log(n)+1} - 1= \Theta(n) $

\end{enumerate}

\section{Exercises 12/2}
%Opgave 1, 2, 3 og 7 fra juni2024
%Opgave 1, 2 og 3 fra AD-April-2024
%Opgave 1, task 1-6 fra Opgavesamling Download Opgavesamling. 



\subsection{Opgave 1, task 1-6 fra Opgavesamling}

\subsubsection{1.1 Recurrence Analysis}
The CPS Brewing Company (CPSBC) has a wide selection of beers, and every time they introduce a new one they always sell it for cheap. The next year they up the price a bit, and again the following year, and so on. In this devious pricing scheme the price of a beer in 2022 may depend on the price of that same beer in the previous years. Your goal is to figure out the \textit{asymptotic behaviour} of the price of different beers.

We denote the price of a beer the year it is introduced and the following years by $p(1),p(2),\ldots$, respectively. The price for any beer the first two years are $p(1)=1$ and $p(2)=2$. The CPC (Chief Price Consultant) of CPSBC then decides on a recursive formula (called a \textit{pricing scheme}).

\paragraph{Task 1:} For the following pricing schemes, use the master method:
\begin{enumerate}
    \item $p(n) = 8p(n/2) + n^2$ vi er i case 1. $f(n) = n^2 = O(n^3-\epsilon)$ dvs. $\Theta(n) = n^3$
    \item $p(n) = 8p(n/4) + n^3$ vi er i case 3  $f(n) = n^3 = \Omega(n^{\frac{3}{2} + \epsilon})$ og for $\frac{4}{8} \leq  c < 1$ gælder $8(\frac{n}{4})^3 \leq c*n^3$
    \item $p(n) = 10p(n/9) + n\log_2 n$ vi er i case 3 $f(n) = nlogn = \Omega(n^{log_{9}10})$ dvs $T(n) = \Theta(n log n)$
\end{enumerate}

\paragraph{Task 2:} For the following pricing schemes, use the substitution method:
\begin{enumerate}
    \item $p(n) = p(n/2) + p(n/3) + n$ 
    
    Let's prove that $p(n) \leq cn^2$ for some constant $c > 0$.
    
    Substituting the inductive hypothesis:
    \begin{align*}
        p(n) &= p(n/2) + p(n/3) + n \\
        &\leq c(n/2)^2 + c(n/3)^2 + n \\
        &= cn^2/4 + cn^2/9 + n \\
        &= cn^2(1/4 + 1/9) + n \\
        &= cn^2(13/36) + n \\
        &\leq cn^2
    \end{align*}
    
    The last inequality holds when $cn^2(13/36) + n \leq cn^2$, or equivalently:
    $n \leq cn^2(23/36)$
    
    This is true for $n \geq \frac{36}{23c}$ and sufficiently large $c$.
    
    Therefore, $p(n) = O(n^2)$.
    \item $p(n) = \sqrt{n}\cdot p(\sqrt{n}) + \sqrt{n}$
    
    Let's prove that $p(n) \leq cn - d\sqrt{n}$ for some constants $c,d > 0$.
    
    Substituting:
    \begin{align*}
        p(n) &= \sqrt{n}\cdot p(\sqrt{n}) + \sqrt{n} \\
        &\leq \sqrt{n}\cdot (c\sqrt{n} - d\sqrt{\sqrt{n}}) + \sqrt{n} \\
        &= cn - d\sqrt{n}\sqrt{\sqrt{n}} + \sqrt{n} \\
        &= cn - d n^{1/4} + \sqrt{n} \\
        &\leq cn - d\sqrt{n}
    \end{align*}
    
    The last inequality holds when $-d n^{1/4} + \sqrt{n} \leq -d\sqrt{n}$, or:
    $\sqrt{n} \leq d({\sqrt{n}} - n^{1/4})$
    
    This is true for sufficiently large $n$ and $d$ since $\sqrt{n}$ grows faster than $n^{1/4}$.
    
    Therefore, $p(n) = O(n)$, and this is a tight bound as can be verified by expanding the recurrence.
\end{enumerate}

\subsubsection{1.2 Sorting}
The CPS Brewing Company is also very interested in sorting their $n$ beers. The CEO has heard that quicksort performs extremely well in practice, but does not like that the worst-case running time is $\Theta(n^2)$. He has heard that there is an $O(n \log n)$ alternative called introspective sort (or introsort).

\paragraph{Task 3:} Write pseudo-code for introsort. You may use functions from the book such as HeapSort, InsertionSort, and Randomized-Partition.

\begin{algorithm}
\caption{Introsort}
\begin{algorithmic}[1]
\Function{Introsort}{$A, i, j, maxdepth$}
    \If{$j-i \leq c$}  \Comment{$c$ is a small constant (e.g., 16)}
        \State \Call{InsertionSort}{$A, i, j$}
    \ElsIf{$maxdepth = 0$}
        \State \Call{HeapSort}{$A, i, j$}
    \Else
        \State $q \gets$ \Call{Randomized-Partition}{$A, i, j$}
        \State \Call{Introsort}{$A, i, q-1, maxdepth-1$}
        \State \Call{Introsort}{$A, q+1, j, maxdepth-1$}
    \EndIf
\EndFunction
\end{algorithmic}
\end{algorithm}

\paragraph{Task 4:} Show that the running time of introsort is worst-case $O(n \log n)$.

The worst-case running time is $O(n \log n)$ because:
\begin{itemize}
    \item If the recursion depth reaches $2\log n$, we switch to HeapSort which is $O(n \log n)$
    \item For smaller subarrays ($\leq c$), we use InsertionSort which is $O(n)$ on small inputs
    \item The partitioning takes $O(n)$ time at each level
    \item The maximum recursion depth is $O(\log n)$
\end{itemize}

\paragraph{Task 5:} Discuss why we use heap sort rather than another $O(n \log n)$ sorting algorithm such as merge sort.

Heap sort is chosen over merge sort because:
\begin{itemize}
    \item Heap sort is in-place, requiring only $O(1)$ extra space
    \item Merge sort requires $O(n)$ extra space
    \item When switching from quicksort, we want to maintain the in-place property
    \item Heap sort has good cache performance compared to merge sort
\end{itemize}

\paragraph{Task 6:} Why is it a good idea to run insertion sort on nearly sorted data, when we know its worst-case running time is $\Theta(n^2)$?

Insertion sort is ideal for nearly sorted data because:
\begin{itemize}
    \item It has $O(n)$ running time on nearly sorted arrays
    \item It has very low overhead compared to other sorting algorithms
    \item It works in-place
    \item It has excellent cache performance on small arrays
    \item The small subarrays in introsort are likely to be nearly sorted due to the previous quicksort partitioning
\end{itemize}

// ... existing code ...

\subsection{Opgave 1, 2 og 3 fra AD-April-2024}

\subsection{1. Sortering (4\%)}
Betragt en variant af MERGE-SORT der deler input op i tre dele, der er lige store (op til afrunding), sorterer hver del rekursivt, og derefter fletter de tre dele sammen med to kald til MERGE (CLRS sektion 2.3). Pseudokoden ser således ud:

\begin{algorithm}
\caption{MERGE3-SORT}
\begin{algorithmic}[1]
\State \textbf{if} $p < s$
\State \quad $q = \lfloor(2 \cdot p + s)/3\rfloor$
\State \quad $r = \lfloor(p + 2 \cdot s)/3\rfloor$
\State \quad MERGE3-SORT$(A,p,q)$
\State \quad MERGE3-SORT$(A,q+1,r)$
\State \quad MERGE3-SORT$(A,r+1,s)$
\State \quad MERGE$(A,p,q,r)$
\State \quad MERGE$(A,p,r,s)$
\end{algorithmic}
\end{algorithm}

Hvilken rekursionsligning beskriver køretiden $T(n)$ af MERGE3-SORT på et input af størrelse $n$ (hvor $n = s - p + 1$)? Der er præcis ét korrekt svar, men det er tilladt at vælge flere svar hvis du er i tvivl.

\begin{enumerate}
    \item $T(n) = T(n/2) + \Theta(n)$
    \item $T(n) = T(2n/3) + \Theta(n)$
    \item $T(n) = 2 \cdot T(n/3) + \Theta(n)$
    \item $T(n) = 2 \cdot T(2n/3) + \Theta(n)$
    \item $T(n) = 3 \cdot T(n/2) + \Theta(n)$
    \item $T(n) = 3 \cdot T(n/3) + \Theta(n)$ korrekt
\end{enumerate}

\subsection{2. Rekursionsligninger (4\%)}
Hvilken af nedenstående rekursionsligninger, nummereret $i = 1,\ldots,6$, har en løsning $T_i$ hvor $T_i(n) = O(n^2)$? Antag at $T_i(n) = 1$ for $n \leq 1$. Vælg ét eller flere korrekte svar.

\textbf{Eksempler.} Hvis $T_0(n) = \Theta(n)$ gælder også $T_0(n) = O(n^2)$. Hvis $T_0(n) = \Theta(2^n)$ gælder \textit{ikke} $T_0(n) = O(n^2)$.

\begin{enumerate}
    \item $T_1(n) = T_1(n/2) + \Theta(n^2)$ ja
    \item $T_2(n) = 4 \cdot T_2(n/2) + \Theta(n^2)$ nej
    \item $T_3(n) = 3 \cdot T_3(n/2) + \Theta(n)$  ja
    \item $T_4(n) = T_4(n/3) + \Theta(n)$ ja
    \item $T_5(n) = 2 \cdot T_5(n/3) + \Theta(n)$ ja
    \item $T_6(n) = 2 \cdot T_6(n-2) + \Theta(n)$ ja
\end{enumerate}

\subsection{3. Nedre grænser for sortering (4\%)}
MERGE-SORT og (deterministisk) QUICKSORT som beskrevet i CLRS sektion 2.3 og 7.1 er eksempler på sammenligningsbaserede sorteringsalgoritmer (eng., comparison sort algorithms). Hvilke konklusioner kan drages som følge af den nedre grænse for sortering, Theorem 8.1 i CLRS? (Du skal ikke tage stilling til om konklusionen er sand, blot om den er en logisk konsekvens af Theorem 8.1.)

\begin{enumerate}
    \item Der findes et input hvor QUICKSORT kører i tid $O(n \lg n)$ i værste fald
    \item Der findes et input hvor MERGE-SORT kører i tid $O(n \lg n)$ i værste fald
    \item Der findes et input hvor QUICKSORT kører i tid $\Omega(n \lg n)$ i værste fald, ja
    \item Der findes et input hvor MERGE-SORT kører i tid $\Omega(n \lg n)$ i værste fald, ja
    \item I gennemsnit over alle input kører MERGE-SORT i tid $\Omega(n \lg n)$ ja
    \item I gennemsnit over alle input kører QUICKSORT i tid $O(n \lg n)$
\end{enumerate}


\section{Exercises 17/2}

\begin{enumerate}
    \item CLRS 26.1-2: Suppose that line 4 of P-FIB(n) rather than calling it is done in parallel with line 5. What would the impact on the asymptotic work, span, and parallelism be?
    
   as per the current implementation we end up asymptotically doing the same work as for every recursive call, one is done in parallel and one is done sequentially as thus we are bound by the sequential call.

   with both recursive calls being parallel we do the same work + overhead but with logical depth of $\Theta(log n)$


    \item CLRS 26.1-4: Prove that $T_P \geq \frac{T_1}{P} + T_{\infty}$.
    
    \begin{proof}
    Let's prove this inequality in two steps:
    
    1) From the work law, we know that $T_P \geq \frac{T_1}{P}$
    
    2) From the span law, we know that $T_P \geq T_{\infty}$
    
    Therefore, for any execution time $T_P$, it must be at least as large as both $\frac{T_1}{P}$ and $T_{\infty}$.
    
    Furthermore, since these lower bounds come from different constraints (total work vs critical path length), 
    the actual execution time must be at least as large as the sum of these bounds.
    
    If this weren't true, consider a schedule that took less time than $\frac{T_1}{P} + T_{\infty}$. 
    This schedule would either:
    \begin{itemize}
        \item Complete more work per time step than the P processors are capable of (violating the work law), or
        \item Complete the critical path faster than sequential dependencies allow (violating the span law)
    \end{itemize}
    
    Since neither of these is possible, we must have:
    $T_P \geq \frac{T_1}{P} + T_{\infty}$
    \end{proof}
    
    \item CLRS 26.1-6: Professor Karan measures her deterministic task-parallel algorithm on 4, 10, and 64 processors of an ideal parallel computer using a greedy scheduler. She claims that the three runs yielded $T_4 = 80$ seconds, $T_{10} = 42$ seconds, and $T_{64} = 10$ seconds. Argue that the professor is either lying or incompetent. (Hint: Use the work law (26.2), the span law (26.3), and inequality (26.5) from Exercise 26.1-4.)
    
    Let's prove this using the laws mentioned:
    
    \begin{enumerate}
        \item From the work law, we know that $T_1 \leq P \cdot T_P$ for any number of processors $P$
        \item Using $P=4$: $T_1 \leq 4 \cdot 80 = 320$ seconds
        \item Using $P=10$: $T_1 \leq 10 \cdot 42 = 420$ seconds
        \item Using $P=64$: $T_1 \leq 64 \cdot 10 = 640$ seconds
    \end{enumerate}
    
    Since $T_1$ is a constant (the sequential runtime), these inequalities must all be consistent. However, from the span law, we know that $T_{\infty} \leq T_P$ for any $P$. Therefore:
    
    \begin{enumerate}
        \item $T_{\infty} \leq T_{64} = 10$ seconds
        \item From Exercise 26.1-4, we know that $T_P \geq \frac{T_1}{P} + T_{\infty}$
        \item For $P=4$: $80 \geq \frac{320}{4} + 10 = 90$
    \end{enumerate}
    
    This is a contradiction! The measured time $T_4 = 80$ cannot satisfy both the work law and the span law with $T_{\infty} \leq 10$. Therefore, Professor Karan's measurements must be incorrect.
    
    \item CLRS 26.1-7: Give a parallel algorithm to multiply an $n \times n$ matrix by an $n$-vector that achieves $\Theta(n^2/\lg n)$ parallelism while maintaining $\Theta(n^2)$ work.
    
    A parallel algorithm that achieves $\Theta(n^2/\lg n)$ parallelism:

    1. For each output element $i$ in parallel:
       - Compute all $n$ products $A[i,j] \cdot x[j]$ in parallel
       - Use parallel reduction to sum the $n$ products in $\Theta(\lg n)$ time

    This gives:
    - Work: $\Theta(n^2)$ ($n$ outputs $\times$ $n$ operations each)
    - Span: $\Theta(\lg n)$ (dominated by the parallel reduction step)
    - Parallelism: $\Theta(n^2)/\Theta(\lg n) = \Theta(n^2/\lg n)$

    \item CLRS 26.1-10: For what number of processors do the two versions of the chess program run equally fast, assuming that $T_P = T_1/P + T_{\infty}$?
    
    \item CLRS 26.2-3 give pseudocode for a parallel algortihm that multiplies two n x n matrices with work $\Theta(n^3)$ but span $\Theta(lg n), explain your algorithm$
    
    we use the solution of 26.1-7 as a subroutine but extend parallelism to be over the n vectors of dim n in parallel. 

\end{enumerate}



\section{Exercises 19/2}

\begin{enumerate}
    \item[16.1-1] If the set of stack operations included a MULTIPUSH operation(additional to Push, Pop, MultiPop), which pushes $k$ elements onto the stack, would the $O(1)$ bound on the amortized cost of stack operations continue to hold?
    
    No, as MultiPush is not similarly restricted as Pop and multipop to be less than or equal to the number of push iteration. So now in the worst case we have n sequences of multipush, each multipush being O(n) for a total of $O(n^2)$

    \item[16.1-3] Suppose we perform a sequence of $n$ operations on a data structure in which the $i$th operation costs $i$ if $i$ is an exact power of 2, and 1 otherwise. Use aggregate analysis to determine the amortized cost per operation.
    
    Let's analyze the total cost for $n$ operations:

    1) Operations at powers of 2 cost $i$: at positions 1, 2, 4, 8, ..., $2^{\lfloor \log_2 n \rfloor}$
    2) All other operations cost 1

    Total cost = $\sum_{i=0}^{\lfloor \log_2 n \rfloor} 2^i + (n - (\lfloor \log_2 n \rfloor + 1))$

    Using the formula for geometric series:
    $= (2^{\lfloor \log_2 n \rfloor + 1} - 1) + (n - \lfloor \log_2 n \rfloor - 1)$
    $= 2^{\lfloor \log_2 n \rfloor + 1} + n - \lfloor \log_2 n \rfloor - 2$
    $\leq 2n + n - \log_2 n - 2$
    $= 3n - \log_2 n - 2$

    Therefore, the amortized cost per operation is:
    $\frac{3n - \log_2 n - 2}{n} = O(1)$

    \item[16.2-2] Redo Exercise 16.1-3 using a potential method of analysis.

    Let's solve this using the potential method. We need to find a potential function $\Phi(Di)$ where Di represents the data structure after i operations.

    Let's define our potential function as:
    \[ \Phi(D_i) = 2\lfloor \log_2 i \rfloor - \log_2 i \]

    For the ith operation:
    \begin{itemize}
        \item If i is a power of 2 (i.e., i = 2k for some k):
            \begin{align*}
                c_i &= i \\
                \Phi(D_i) - \Phi(D_{i-1}) &= (2k - k) - (2(k-1) - (k-1)) \\
                &= k - (k-1) = 1
            \end{align*}
            Therefore, amortized cost = actual cost + potential change
            \[ \hat{c_i} = i + 1 \]

        \item If i is not a power of 2:
            \begin{align*}
                c_i &= 1 \\
                \Phi(D_i) - \Phi(D_{i-1}) &\leq 0
            \end{align*}
            Therefore,
            \[ \hat{c_i} \leq 1 \]
    \end{itemize}

    Since $\Phi(D_0) = 0$ and $\Phi(D_n) \geq 0$, and we've shown that the amortized cost of each operation is O(1), the amortized cost per operation is O(1).
    
    \item[16.3-3] Consider an ordinary binary min-heap data structure with the usual instructions INSERT and EXTRACT-MIN that operates on a heap of size n. Give a potential function $\Theta$ such that the amortized cost of INSERT is O(1) and the amortized cost of EXTRACT-MIN is O(lg n).
    
    Let's define the potential function as: $\Phi(D_n) = 2n\lg n$, where n is the current size of the heap.

    Analysis:
    \begin{itemize}
        \item For INSERT:
            \begin{itemize}
                \item Actual cost: $O(\lg n)$ for bubble-up
                \item Potential change: $2(n+1)\lg(n+1) - 2n\lg n = O(\lg n)$
                \item Amortized cost = actual - potential change = $O(\lg n) - O(\lg n) = O(1)$
            \end{itemize}
        \item For EXTRACT-MIN:
            \begin{itemize}
                \item Actual cost: $O(\lg n)$ for bubble-down
                \item Potential change: $2(n-1)\lg(n-1) - 2n\lg n = -O(\lg n)$
                \item Amortized cost = actual - potential change = $O(\lg n) + O(\lg n) = O(\lg n)$
            \end{itemize}
    \end{itemize}

    This potential function works because:
    1. It's always non-negative (required for potential functions)
    2. The potential decreases during INSERT to offset its actual cost
    3. The potential increases during EXTRACT-MIN, maintaining its logarithmic cost

    \item[16.3-4] Show that for any comparison-based implementation of a binary min-heap data structure, the amortized cost of EXTRACT-MIN must be $\Omega(\lg n)$, where $n$ is the heap size. Use a potential function argument.
    
    TODO
    
    \item[16.3-2] Red-black trees are an implementation of dynamic search trees where the amortized cost of all operations is O(lg n). Show that in any sequence of operations on a red-black tree, the actual cost of any operation is O(n).
    
    first of all, we consider the following operations
    1. insert
    2. delete
    
    \item[16.4-4] Suppose that instead of contracting the table by halving its size when its load factor drops below 1/4, we contract it by multiplying its size by 2/3 when its load factor drops below 1/3. Using the potential method, show that the amortized cost of a TABLE-DELETE that uses this new rule is bounded above by a constant.
    
    \item Opgave 8: juni 2024 AD exam
    Vi betragter trærepræsentationer af disjunkte mængder (eng. disjoint sets), som beskrevet i CLRS sektion 19.3, ved brug af heuristikken union by rank. Antag at der laves $k_1$ MAKE-SET operationer, $k_2$ UNION operationer, og $k_3$ FIND-SET operationer. Hvilke af nedenstående udsagn om den totale køretid for disse operationer er korrekte for alle værdier af $k_1, k_2, k_3$? Vælg ét eller flere korrekte svar.

    \begin{enumerate}
        \item Tiden er $O(\log k_1 + \log k_2 + \log k_3)$
        \item Tiden er $O(k_1 + \log k_2 + \log k_3)$
        \item Tiden er $O(k_1 + k_2 + \log k_3)$
        \item Tiden er $O(k_1 + k_2 + k_3)$
        \item Tiden er $O((k_1 + k_2 + k_3) \log k_1)$
        \item Tiden er $O((k_1 + k_2 + k_3) \log(k_1 + k_2 + k_3))$
    \end{enumerate}
    
    
    \item Opgave 6: april 2024 AD exam - Amortiseret analyse
    
    Betragt dynamiske tabeller (eng., dynamic tables) som beskrevet i CLRS sektion 16.4.2, altså versionen med en TABLE-DELETE operation. Vi betragter potentialfunktionen defineret i CLRS ligning (16.5), hvor $\alpha(T) = T.num/T.size$:

    \[ \Phi(T) = \begin{cases}
    2(T.num - T.size/2) & \text{hvis } \alpha(T) \geq 1/2, \\
    T.size - T.num & \text{hvis } \alpha(T) < 1/2 .
    \end{cases} \]

    Hvilke af følgende udsagn er sande? Vælg ét eller flere korrekte svar.

    \begin{enumerate}
        \item En TABLE-INSERT operation øger altid værdien af $\Phi(T)$
        \item En TABLE-DELETE operation øger altid værdien af $\Phi(T)$
        \item En TABLE-INSERT operation mindsker altid værdien af $\Phi(T)$
        \item En TABLE-DELETE operation mindsker altid værdien af $\Phi(T)$
        \item Værdien af $\Phi(T)$ er altid større eller lig med nul
        \item Værdien af $\Phi(T)$ er altid mindre end eller lig med $T.size$
    \end{enumerate}

\end{enumerate}


\section{Exercises 24/2}


\subsection{19.3-a FIB-HEAP-CHANGE-KEY Implementation}
The operation FIB-HEAP-CHANGE-KEY(H,x,k) changes the key of node x to the value k. Give an efficient implementation of FIB-HEAP-CHANGE-KEY, and analyze the amortized running time of your implementation for the cases in which k is greater than, less than, or equal to x.key.





\subsection{19.4-1 Fibonacci Heap Height}
Professor Pinocchio claims that a Fibonacci heap with n nodes always has height O(lg n). Show that the professor is mistaken by constructing a sequence of Fibonacci-heap operations that creates a Fibonacci heap consisting of just one tree that has height $\omega(lg n)$.

We can construct a Fibonacci heap with height $\Omega(\sqrt{n})$ using the following sequence of operations:

1. Insert n nodes with keys $1, 2, ..., n$ into the heap
2. Extract-min repeatedly until only $\sqrt{n}$ nodes remain as roots
3. Perform unions in a way that creates a single path:
   - Let $x_1, x_2, ..., x_{\sqrt{n}}$ be the remaining roots
   - For $i = 1$ to $\sqrt{n}-1$:
     - Link $x_i$ with $x_{i+1}$ making $x_i$ a child of $x_{i+1}$

This creates a single tree with height $\Omega(\sqrt{n})$, which is $\omega(\lg n)$.

The key observation is that while Fibonacci heaps maintain the minimum-heap property and degree bounds, they do not guarantee balanced trees. The sequence above exploits this by creating a highly unbalanced path-like structure.

Therefore, Professor Pinocchio's claim that Fibonacci heaps always have height O(lg n) is incorrect.




\subsection{7 Fibonacci- Heap AD april 2024}

answers 
1, 4,5,6

\subsection{8 time complexity for Fibonacci- Heap AD april 2024}

answers

1. yes upper bounded by O(n)
3. yes upper bounded by constant cost + change in potential.

\subsection{17 shortest paths AD april 2024}

everything except {B, E} and {D, E}


\subsection{9 AD-June-2024}

2,3,4 as we expand the root node list


\subsection{17 AD-June-2024}




\bibliographystyle{plainnat}
\bibliography{references}
\end{document}

